{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RP3UA_zs3Og3",
        "outputId": "452b60bf-0d4c-4016-fa00-b8d037f692c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 complete\n",
            "Epoch 2 complete\n",
            "Epoch 3 complete\n",
            "Accuracy: 0.9360859864206713\n",
            "F1 Score: 0.9355395413416894\n",
            "\n",
            "Sample Predictions:\n",
            "Text: im still feeling unsure...\n",
            "Actual: 4 | Predicted: 4\n",
            "\n",
            "Text: i feel so welcomed at...\n",
            "Actual: 1 | Predicted: 1\n",
            "\n",
            "Text: i just say that changing grades at my age has truly kicked my b i...\n",
            "Actual: 5 | Predicted: 5\n",
            "\n",
            "Text: i really feel valued...\n",
            "Actual: 1 | Predicted: 1\n",
            "\n",
            "Text: i feel that even if we mess up while trying to obey him he is...\n",
            "Actual: 1 | Predicted: 1\n",
            "\n",
            "Text: i must admit i was feeling apprehensive about our visit to <UNK>...\n",
            "Actual: 4 | Predicted: 4\n",
            "\n",
            "Text: i ended up feeling jealous...\n",
            "Actual: 3 | Predicted: 3\n",
            "\n",
            "Text: i feel doomed to fail because of the unreasonable <UNK> and poor materials ive been...\n",
            "Actual: 0 | Predicted: 0\n",
            "\n",
            "Text: i feel so stupid feeling such rage within me whenever i see those girls whom...\n",
            "Actual: 0 | Predicted: 0\n",
            "\n",
            "Text: i was reading i found myself feeling agitated frustrated angry and unbelievably sad that i...\n",
            "Actual: 4 | Predicted: 3\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 1. Load Data\n",
        "df = pd.read_csv(\"/content/emotions.csv\")\n",
        "df['text'] = df['text'].astype(str)\n",
        "\n",
        "# 2. Label encode emotion\n",
        "le = LabelEncoder()\n",
        "df['label'] = le.fit_transform(df['label'])\n",
        "\n",
        "# 3. Tokenize\n",
        "def tokenize(text):\n",
        "    return text.lower().split()\n",
        "\n",
        "# 4. Build vocabulary with proper special tokens\n",
        "tokens = [token for text in df['text'] for token in tokenize(text)]\n",
        "vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "vocab.update({w: i + 2 for i, (w, _) in enumerate(Counter(tokens).most_common(8000))})\n",
        "\n",
        "# 5. Encode text with <UNK> handling\n",
        "def encode(text):\n",
        "    return [vocab.get(w, vocab[\"<UNK>\"]) for w in tokenize(text)]\n",
        "\n",
        "df['input_ids'] = df['text'].apply(encode)\n",
        "\n",
        "# 6. Prepare splits\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['input_ids'], df['label'], test_size=0.2, stratify=df['label'])\n",
        "\n",
        "# 7. Dataset & DataLoader\n",
        "class EmotionDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = [torch.tensor(x) for x in texts]\n",
        "        self.labels = torch.tensor(labels.values)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx], self.labels[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "def pad_collate(batch):\n",
        "    texts, labels = zip(*batch)\n",
        "    padded = pad_sequence(texts, batch_first=True)\n",
        "    return padded, torch.tensor(labels)\n",
        "\n",
        "train_dl = DataLoader(EmotionDataset(X_train, y_train), batch_size=64, shuffle=True, collate_fn=pad_collate)\n",
        "test_dl  = DataLoader(EmotionDataset(X_test,  y_test),  batch_size=64, collate_fn=pad_collate)\n",
        "\n",
        "# 8. Model\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, out_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.emb(x)\n",
        "        _, (h, _) = self.lstm(x)\n",
        "        return self.fc(h[-1])\n",
        "\n",
        "model = LSTM(len(vocab), 100, 128, len(le.classes_))\n",
        "opt = torch.optim.Adam(model.parameters())\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# 9. Train\n",
        "for epoch in range(3):\n",
        "    model.train()\n",
        "    for X, y in train_dl:\n",
        "        opt.zero_grad()\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "    print(f\"Epoch {epoch+1} complete\")\n",
        "\n",
        "# 10. Evaluate\n",
        "model.eval()\n",
        "all_preds, all_true = [], []\n",
        "with torch.no_grad():\n",
        "    for X, y in test_dl:\n",
        "        logits = model(X)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        all_preds.extend(preds.tolist())\n",
        "        all_true.extend(y.tolist())\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(all_true, all_preds))\n",
        "print(\"F1 Score:\", f1_score(all_true, all_preds, average='weighted'))\n",
        "\n",
        "# 11. Sample Predictions â€” Improved text display\n",
        "idx2word = {idx: word for word, idx in vocab.items()}\n",
        "print(\"\\nSample Predictions:\")\n",
        "X_test_ = X_test.reset_index(drop=True)\n",
        "y_test_ = y_test.reset_index(drop=True)\n",
        "\n",
        "for i in range(10):\n",
        "    text_tokens = X_test_.iloc[i]\n",
        "    text_tensor = torch.tensor(text_tokens).unsqueeze(0)\n",
        "    padded = pad_sequence([text_tensor.squeeze()], batch_first=True)\n",
        "    with torch.no_grad():\n",
        "        output = model(padded)\n",
        "        pred_label = output.argmax(dim=1).item()\n",
        "\n",
        "    input_words = [idx2word.get(idx, '<UNK>') for idx in text_tokens][:15]\n",
        "    actual_label = le.inverse_transform([y_test_.iloc[i]])[0]\n",
        "    predicted_label = le.inverse_transform([pred_label])[0]\n",
        "    print(f\"Text: {' '.join(input_words)}...\")\n",
        "    print(f\"Actual: {actual_label} | Predicted: {predicted_label}\\n\")"
      ]
    }
  ]
}